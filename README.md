# LSH_Attention
Calculate Softmax layer of Attention in O(LlogL)(L=sequence length) insread of O(L^2) using polytope Locality-Sensitive Hashing(https://arxiv.org/abs/1802.05751 ). 
